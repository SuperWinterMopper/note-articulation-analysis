{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d863e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "from numpy.fft import fft, fftshift, fftfreq\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b363c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "importlib.reload(helpers)\n",
    "\n",
    "# Driver\n",
    "class Ex:\n",
    "    def __init__(self, name: str, num_notes: int, spec_thresh: float, bpm: int, max_num_notes_per_beat: int):\n",
    "        self.name = name\n",
    "        self.num_notes = num_notes\n",
    "        self.spec_thresh = spec_thresh\n",
    "        self.bpm = bpm\n",
    "        self.max_num_notes_per_beat = max_num_notes_per_beat\n",
    "        self.min_time_between = 60 / (self.bpm * max_num_notes_per_beat)\n",
    "        self.sustain_thresh_coeff = 0\n",
    "\n",
    "def getExercises():\n",
    "    return [\n",
    "        Ex(\"ex1WholeMod.mp4\", num_notes=90, spec_thresh=.15, bpm=80, max_num_notes_per_beat=2),\n",
    "        Ex(\"ex1WholeModF.mp4\", num_notes=90, spec_thresh=.15, bpm=120, max_num_notes_per_beat=2),\n",
    "        Ex(\"ex2WholeMod.mp4\", num_notes=49, spec_thresh=.15, bpm=80, max_num_notes_per_beat=1),\n",
    "        Ex(\"ex3WholeMod.mp4\", num_notes=145, spec_thresh=.15, bpm=100, max_num_notes_per_beat=2),\n",
    "        Ex(\"ex3WholeModF.mp4\", num_notes=145, spec_thresh=.15, bpm=130, max_num_notes_per_beat=2),\n",
    "        Ex(\"ex4WholeMod.mp4\", num_notes=102, spec_thresh=.15, bpm=90, max_num_notes_per_beat=4),\n",
    "        Ex(\"ex4WholeModF.mp4\", num_notes=102, spec_thresh=.05, bpm=120, max_num_notes_per_beat=2),\n",
    "        Ex(\"ex5WholeMod.mp4\", num_notes=133, spec_thresh=.15, bpm=64, max_num_notes_per_beat=2),\n",
    "        Ex(\"ex5WholeModF.mp4\", num_notes=133, spec_thresh=.15, bpm=86, max_num_notes_per_beat=2),\n",
    "        Ex(\"ex6WholeMod.mp4\", num_notes=118, spec_thresh=.15, bpm=70, max_num_notes_per_beat=4),\n",
    "        Ex(\"ex6WholeModF.mp4\", num_notes=118, spec_thresh=.15, bpm=90, max_num_notes_per_beat=4),\n",
    "        Ex(\"ex7WholeMod.mp4\", num_notes=86, spec_thresh=.15, bpm=70, max_num_notes_per_beat=2),\n",
    "        Ex(\"ex8WholeMod.mp4\", num_notes=112, spec_thresh=.15, bpm=55, max_num_notes_per_beat=4),\n",
    "        Ex(\"ex8WholeModF.mp4\", num_notes=112, spec_thresh=.15, bpm=80, max_num_notes_per_beat=4),\n",
    "        Ex(\"ex9WholeMod.mp4\", num_notes=121, spec_thresh=.15, bpm=100, max_num_notes_per_beat=4),\n",
    "        Ex(\"ex10WholeMod.mp4\", num_notes=102, spec_thresh=.15, bpm=120, max_num_notes_per_beat=3)\n",
    "    ]\n",
    "\n",
    "def observe_accuracy(exercises):\n",
    "    avg_artic_leng = 0\n",
    "\n",
    "    for ex in exercises:\n",
    "        exercise = ex.name\n",
    "        note_count = ex.num_notes\n",
    "        spec_thresh = ex.spec_thresh\n",
    "        min_time_between = ex.min_time_between * .8\n",
    "        sustain_thresh = ex.sustain_thresh_coeff * spec_thresh\n",
    "\n",
    "        ys, ts, sr = helpers.get_audio_data(f\"exercises/{exercise}\")\n",
    "        ts_fr, fr_freq_amps, freq_bins = helpers.magnitude_spectrogram(ys, ts, sr)\n",
    "        ts_fr, spec_flux = helpers.compute_spectral_flux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "        centroids = helpers.compute_spectral_centroid(time_frames=ts_fr, freq_bins=freq_bins, frame_freq_amps=fr_freq_amps)\n",
    "        \n",
    "        plt.title(exercise)\n",
    "        plt.xlabel(\"Time (s)\"); plt.ylabel(\"Spectral Flux\")\n",
    "        plt.plot(ts_fr, spec_flux, c=\"orange\")\n",
    "        img_path = Path.cwd() / \"spec_flux_graphs\" / f\"{exercise} Spectral Flux.png\"\n",
    "        plt.savefig(img_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # onsets = helpers.detect_onsets(spec_flux, ts_fr, threshold=spec_thresh, min_time_between=min_time_between)\n",
    "        onsets, sustains = helpers.detect_onsets_and_release(spec_flux=spec_flux, times=ts_fr, sr=sr, sustain_thresh=sustain_thresh, onset_thresh=spec_thresh, min_time_between=min_time_between)\n",
    "\n",
    "        print_num_onset_detection_accuracy = True\n",
    "        graph_onsets_sustains = True\n",
    "        \n",
    "        if print_num_onset_detection_accuracy:\n",
    "            print(f\"For exercise {exercise}, Note count Actual:\\t{note_count}, Note count Detected: \\t{len(onsets)} | Accuracy: \\t{min(note_count / len(onsets), len(onsets) / note_count)}\")\n",
    "\n",
    "        if graph_onsets_sustains:\n",
    "            artic_lens = [sustains[i] - onsets[i] for i in range(min(len(onsets), len(sustains)))]\n",
    "            # if np.max(artic_lens) > 1:\n",
    "            # print(f\"For {exercise}, min articulation length: {np.min(artic_lens)}\")\n",
    "            # if np.max(artic_lens) > .1:\n",
    "            #     print(\"==============================================================\")\n",
    "            #     print(f\"For {exercise}, max articulation length: {np.max(artic_lens)}\")\n",
    "            print(f\"For {exercise}, mean articulation length: {np.average(artic_lens)}\")\n",
    "            avg_artic_leng += np.average(artic_lens)\n",
    "\n",
    "            onset_indic = np.zeros_like(ts_fr)\n",
    "            sustain_indic = np.zeros_like(ts_fr)\n",
    "\n",
    "            stem_top = np.max(centroids)\n",
    "\n",
    "            # Set 1.0 at the closest time points where onsets/sustains occur\n",
    "            for onset_time in onsets:\n",
    "                idx = np.argmin(np.abs(ts_fr - onset_time))\n",
    "                onset_indic[idx] = stem_top\n",
    "                \n",
    "            for sustain_time in sustains:\n",
    "                idx = np.argmin(np.abs(ts_fr - sustain_time))\n",
    "                sustain_indic[idx] = stem_top\n",
    "\n",
    "            plt.title(f\"{exercise} Onset + Sustain + Spectral Centroid\")\n",
    "            plt.xlim(10, 14)\n",
    "            plt.xlabel(\"Times (s)\")\n",
    "            plt.stem(ts_fr, onset_indic, linefmt='--', markerfmt='pink', label='Onsets')\n",
    "            plt.stem(ts_fr, sustain_indic, linefmt='--', markerfmt='red', label='Sustains')\n",
    "            plt.legend()\n",
    "            img_path = Path.cwd() / \"centroids\" / f\"{exercise} centroid.png\"\n",
    "            plt.plot(ts_fr, centroids)\n",
    "            plt.savefig(img_path)\n",
    "            plt.close()\n",
    "\n",
    "    print(f\"Average articulation window size is {avg_artic_leng / len(exercises)}\")\n",
    "\n",
    "# observe_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977a7e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for exercise ex1WholeMod.mp4, note onset spectral threshold is 0.11666666666666665\n",
      "for exercise ex1WholeModF.mp4, note onset spectral threshold is 0.21606060606060606\n",
      "for exercise ex2WholeMod.mp4, note onset spectral threshold is 0.14333333333333334\n",
      "for exercise ex3WholeMod.mp4, note onset spectral threshold is 0.1578787878787879\n",
      "for exercise ex3WholeModF.mp4, note onset spectral threshold is 0.10454545454545454\n",
      "for exercise ex4WholeMod.mp4, note onset spectral threshold is 0.19424242424242424\n",
      "for exercise ex4WholeModF.mp4, note onset spectral threshold is 0.08757575757575757\n",
      "for exercise ex5WholeMod.mp4, note onset spectral threshold is 0.14333333333333334\n",
      "for exercise ex5WholeModF.mp4, note onset spectral threshold is 0.1384848484848485\n",
      "for exercise ex6WholeMod.mp4, note onset spectral threshold is 0.11424242424242424\n",
      "for exercise ex6WholeModF.mp4, note onset spectral threshold is 0.14333333333333334\n",
      "for exercise ex7WholeMod.mp4, note onset spectral threshold is 0.14575757575757575\n",
      "for exercise ex8WholeMod.mp4, note onset spectral threshold is 0.18454545454545457\n",
      "for exercise ex8WholeModF.mp4, note onset spectral threshold is 0.1603030303030303\n",
      "for exercise ex9WholeMod.mp4, note onset spectral threshold is 0.10212121212121211\n",
      "for exercise ex10WholeMod.mp4, note onset spectral threshold is 0.1990909090909091\n",
      "[<__main__.Ex object at 0x000001233AB39720>, <__main__.Ex object at 0x000001233AB3AE60>, <__main__.Ex object at 0x000001233AB3B040>, <__main__.Ex object at 0x000001233AB3AE30>, <__main__.Ex object at 0x000001233AB3BE80>, <__main__.Ex object at 0x000001233AB39C90>, <__main__.Ex object at 0x000001233AB39C60>, <__main__.Ex object at 0x000001233AB3B580>, <__main__.Ex object at 0x000001233AB3B670>, <__main__.Ex object at 0x000001233AB3B6A0>, <__main__.Ex object at 0x000001233AB3B640>, <__main__.Ex object at 0x000001233AB3B5E0>, <__main__.Ex object at 0x000001233AB3B610>, <__main__.Ex object at 0x000001233AB3B5B0>, <__main__.Ex object at 0x000001233AB3B550>, <__main__.Ex object at 0x000001233AB3B4F0>]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helpers)\n",
    "\n",
    "def optimize_spec_thresh():\n",
    "    lowest, highest, num_tests = .01, .25, 100\n",
    "    spec_thres_vals = np.linspace(lowest, highest, num_tests)\n",
    "    opt_spec_thresh = [] # the optimized thresholds for each exercise\n",
    "\n",
    "    exerciseNoteCounts = getExercises()\n",
    "\n",
    "    for ex in exerciseNoteCounts:\n",
    "        exercise = ex.name\n",
    "        note_count = ex.num_notes\n",
    "        min_time_between = ex.min_time_between * .8\n",
    "\n",
    "        num_onsets_detected = []\n",
    "        accuracies = []\n",
    "\n",
    "        ys, ts, sr = helpers.get_audio_data(f\"exercises/{exercise}\")\n",
    "        ts_fr, fr_freq_amps, _ = helpers.magnitude_spectrogram(ys, ts, sr)\n",
    "        ts_fr, spec_flux = helpers.compute_spectral_flux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "        for spec_thresh in spec_thres_vals:\n",
    "            onsets = helpers.detect_onsets_only(spec_flux=spec_flux, times=ts_fr, sr=sr, onset_thresh=spec_thresh, min_time_between=min_time_between)\n",
    "            accuracy = min(note_count / len(onsets), len(onsets) / note_count)\n",
    "\n",
    "            num_onsets_detected.append(len(onsets))\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        plt.title(f\"{exercise} Spectral Flux Threshold vs Number of Onsets Detected\")\n",
    "        plt.xlabel(\"Spectral Flux Threshold\"); plt.ylabel(\"Number of Onsets Detected\")\n",
    "        plt.plot(spec_thres_vals, num_onsets_detected, label=\"Threshold vs Onsets\")\n",
    "        plt.axhline(y=note_count, color='limegreen', linestyle='--', linewidth=2, label=\"Correct # Onsets\")\n",
    "        plt.legend()\n",
    "        img_path = Path.cwd() / \"spec_thresh_opt\" / \"graphs\" / f\"{exercise}.png\"\n",
    "        plt.savefig(img_path)\n",
    "        plt.close()\n",
    "\n",
    "        opt_perf = max(accuracies)\n",
    "        opt_perf_spec_thresh = [spec_thres_vals[i] for i, val in enumerate(accuracies) if val == opt_perf]\n",
    "        with open(\"spec_thresh_opt/optimized_values.txt\", \"a\") as f:\n",
    "            f.write(f\"{'=' * 30}\\n\")\n",
    "            f.write(f\"Best performing spectral thresholds at {opt_perf * 100}% accuracy for {exercise}:\\n\")\n",
    "            f.write(f\"{opt_perf_spec_thresh}\\n\\n\")\n",
    "\n",
    "        opt_spec_thresh.append(opt_perf_spec_thresh[len(opt_perf_spec_thresh) // 2]) # choose the middle most successful output to maximize applicability\n",
    "        \n",
    "    minimum_thresh = .08 # optimizing for the best threshold for this case hurts sustain onset detection, so this ensures a decent middle ground.\n",
    "    for i, ex in enumerate(exerciseNoteCounts):\n",
    "        ex.spec_thresh = max(opt_spec_thresh[i], minimum_thresh)\n",
    "\n",
    "    return exerciseNoteCounts\n",
    "\n",
    "opt_exercises = optimize_spec_thresh()\n",
    "for ex in opt_exercises:\n",
    "    print(f\"for exercise {ex.name}, note onset spectral threshold is {ex.spec_thresh}\")\n",
    "print(opt_exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a7567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For exercise ex1WholeMod.mp4, Note count Actual:\t90, Note count Detected: \t90 | Accuracy: \t1.0\n",
      "For ex1WholeMod.mp4, mean articulation length: 0.043111111111111294\n",
      "For exercise ex1WholeModF.mp4, Note count Actual:\t90, Note count Detected: \t90 | Accuracy: \t1.0\n",
      "For ex1WholeModF.mp4, mean articulation length: 0.03333333333333348\n",
      "For exercise ex2WholeMod.mp4, Note count Actual:\t49, Note count Detected: \t49 | Accuracy: \t1.0\n",
      "For ex2WholeMod.mp4, mean articulation length: 0.03877551020408123\n",
      "For exercise ex3WholeMod.mp4, Note count Actual:\t145, Note count Detected: \t145 | Accuracy: \t1.0\n",
      "For ex3WholeMod.mp4, mean articulation length: 0.038482758620689596\n",
      "For exercise ex3WholeModF.mp4, Note count Actual:\t145, Note count Detected: \t145 | Accuracy: \t1.0\n",
      "For ex3WholeModF.mp4, mean articulation length: 0.03806896551724144\n",
      "For exercise ex4WholeMod.mp4, Note count Actual:\t102, Note count Detected: \t102 | Accuracy: \t1.0\n",
      "For ex4WholeMod.mp4, mean articulation length: 0.03754901960784308\n",
      "For exercise ex4WholeModF.mp4, Note count Actual:\t102, Note count Detected: \t104 | Accuracy: \t0.9807692307692307\n",
      "For ex4WholeModF.mp4, mean articulation length: 0.05201923076923097\n",
      "For exercise ex5WholeMod.mp4, Note count Actual:\t133, Note count Detected: \t133 | Accuracy: \t1.0\n",
      "For ex5WholeMod.mp4, mean articulation length: 0.04541353383458625\n",
      "For exercise ex5WholeModF.mp4, Note count Actual:\t133, Note count Detected: \t133 | Accuracy: \t1.0\n",
      "For ex5WholeModF.mp4, mean articulation length: 0.04195488721804525\n",
      "For exercise ex6WholeMod.mp4, Note count Actual:\t118, Note count Detected: \t118 | Accuracy: \t1.0\n",
      "For ex6WholeMod.mp4, mean articulation length: 0.03957627118644087\n",
      "For exercise ex6WholeModF.mp4, Note count Actual:\t118, Note count Detected: \t118 | Accuracy: \t1.0\n",
      "For ex6WholeModF.mp4, mean articulation length: 0.04084745762711836\n",
      "For exercise ex7WholeMod.mp4, Note count Actual:\t86, Note count Detected: \t86 | Accuracy: \t1.0\n",
      "For ex7WholeMod.mp4, mean articulation length: 0.034767441860465445\n",
      "For exercise ex8WholeMod.mp4, Note count Actual:\t112, Note count Detected: \t112 | Accuracy: \t1.0\n",
      "For ex8WholeMod.mp4, mean articulation length: 0.035892857142857344\n",
      "For exercise ex8WholeModF.mp4, Note count Actual:\t112, Note count Detected: \t112 | Accuracy: \t1.0\n",
      "For ex8WholeModF.mp4, mean articulation length: 0.03830357142857145\n",
      "For exercise ex9WholeMod.mp4, Note count Actual:\t121, Note count Detected: \t27 | Accuracy: \t0.2231404958677686\n",
      "For ex9WholeMod.mp4, mean articulation length: 0.5603703703703704\n",
      "For exercise ex10WholeMod.mp4, Note count Actual:\t102, Note count Detected: \t102 | Accuracy: \t1.0\n",
      "For ex10WholeMod.mp4, mean articulation length: 0.039901960784313725\n",
      "Average articulation window size is 0.07239801753851875\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helpers)\n",
    "\n",
    "def optimize_sustain_thresh_coeff(exercises):\n",
    "    # lowest, highest, num_tests = -.01, .85, 100\n",
    "    lowest, highest, num_tests = 0, 1.5, 100\n",
    "    sustain_thresh_coeffs = np.linspace(lowest, highest, num_tests)\n",
    "    opt_coeffs = [] # the optimized coefficients for each exercise\n",
    "\n",
    "    for ex in exercises:\n",
    "        exercise = ex.name\n",
    "        note_count = ex.num_notes\n",
    "        spec_thresh = ex.spec_thresh\n",
    "        min_time_between = ex.min_time_between * .8\n",
    "\n",
    "        sustain_saturations = []\n",
    "\n",
    "        ys, ts, sr = helpers.get_audio_data(f\"exercises/{exercise}\")\n",
    "        ts_fr, fr_freq_amps, _ = helpers.magnitude_spectrogram(ys, ts, sr)\n",
    "        ts_fr, spec_flux = helpers.compute_spectral_flux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "        for coeff in sustain_thresh_coeffs:\n",
    "            sustain_thresh = spec_thresh * coeff\n",
    "            _, sustains = helpers.detect_onsets_and_release(spec_flux=spec_flux, times=ts_fr, sr=sr, onset_thresh=spec_thresh, sustain_thresh=sustain_thresh, min_time_between=min_time_between)\n",
    "            sustain_saturations.append(len(sustains) / note_count)\n",
    "\n",
    "        plt.title(f\"{exercise} Coefficient for Sustain vs Onset Pairing Completeness\")\n",
    "        plt.xlabel(\"Coefficient for Sustain\"); plt.ylabel(\"Onset Pairing Completeness\")\n",
    "        plt.plot(sustain_thresh_coeffs, sustain_saturations)\n",
    "        img_path = Path.cwd() / \"sustain_thresh_opt\" / \"graphs\" / f\"{exercise}.png\"\n",
    "        plt.savefig(img_path)\n",
    "        plt.close()\n",
    "    \n",
    "        opt_perf_sustain_coeff = [sustain_thresh_coeffs[i] for i, val in enumerate(sustain_saturations) if val == max(sustain_saturations)]\n",
    "        \n",
    "        with open(\"sustain_thresh_opt/optimized_values.txt\", \"a\") as f:\n",
    "            f.write(f\"{'=' * 30}\\n\")\n",
    "            f.write(f\"Minimum coefficient for complete onset + sustain pairing for {exercise}:\\n\")\n",
    "            f.write(f\"{np.min(opt_perf_sustain_coeff)}\\n\\n\")\n",
    "\n",
    "        opt_coeffs.append(np.min(opt_perf_sustain_coeff) * 1.1) # choose the minimum most successful output which is closest to ideal best\n",
    "\n",
    "    for i, ex in enumerate(exercises):\n",
    "        ex.sustain_thresh_coeff = opt_coeffs[i]\n",
    "        \n",
    "    return exercises\n",
    "\n",
    "opt_exercises = optimize_sustain_thresh_coeff(opt_exercises)\n",
    "\n",
    "observe_accuracy(opt_exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeddaf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"opt_results.txt\", \"w\") as f: \n",
    "    f.write(\"FINAL OPTIMIZED PARAMETERS FOR EACH EXERCISE\\n\\n\")\n",
    "    for ex in opt_exercises:\n",
    "        f.write(f\"{'=' * 50}\\n\")\n",
    "        f.write(f\"Name:                             {ex.name:>15}\\n\")\n",
    "        f.write(f\"Note Count:                       {ex.num_notes:>15d}\\n\")\n",
    "        f.write(f\"Spectral Threshold:               {ex.spec_thresh:>15.4f}\\n\")\n",
    "        f.write(f\"BPM:                              {ex.bpm:>15d}\\n\")\n",
    "        f.write(f\"Max Notes Per Beat:               {ex.max_num_notes_per_beat:>15d}\\n\")\n",
    "        f.write(f\"Min Time Between Notes:           {ex.min_time_between:>15.4f} seconds\\n\")\n",
    "        f.write(f\"Sustain Threshold Coefficient:    {ex.sustain_thresh_coeff:>15.4f}\\n\")\n",
    "        f.write(f\"Sustain Threshold Value:          {ex.spec_thresh * ex.sustain_thresh_coeff:>15.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54dac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(helpers)\n",
    "\n",
    "from bisect import bisect_left\n",
    "import os, json\n",
    "\n",
    "def pack_articulations(data, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    amps_path  = os.path.join(out_dir, \"pack.amps.f32\")\n",
    "    cents_path = os.path.join(out_dir, \"pack.cents.f32\")\n",
    "\n",
    "    f_amps  = open(amps_path,  \"wb\")\n",
    "    f_cents = open(cents_path, \"wb\")\n",
    "\n",
    "    manifest = {\n",
    "        \"amps\":  { \"file\": \"pack.amps.f32\",  \"dtype\": \"float32\" },\n",
    "        \"cents\": { \"file\": \"pack.cents.f32\", \"dtype\": \"float32\" },\n",
    "        \"exercises\": []\n",
    "    }\n",
    "\n",
    "    amp_off = 0   # element offsets (float32)\n",
    "    cent_off = 0\n",
    "\n",
    "    for ex_idx, ex in enumerate(data):\n",
    "        try:\n",
    "            onsets, sustains, amps_list, cents_list = ex\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Exercise {ex_idx} must be [onsets, sustains, amps, centroids]\") from e\n",
    "\n",
    "        nA = len(onsets)\n",
    "        if not (len(sustains) == len(amps_list) == len(cents_list) == nA):\n",
    "            raise ValueError(\n",
    "                f\"Exercise {ex_idx}: length mismatch \"\n",
    "                f\"(onsets={len(onsets)}, sustains={len(sustains)}, \"\n",
    "                f\"amps={len(amps_list)}, cents={len(cents_list)})\"\n",
    "            )\n",
    "\n",
    "        ex_entry = { \"id\": f\"ex{ex_idx}\", \"articulations\": [] }\n",
    "\n",
    "        for a_idx in range(nA):\n",
    "            on_t = float(onsets[a_idx])\n",
    "            sus_t = float(sustains[a_idx])\n",
    "\n",
    "            # Convert series to little-endian float32 and append\n",
    "            amp_arr  = np.asarray(amps_list[a_idx],  dtype=np.float32).astype('<f4', copy=False)\n",
    "            cent_arr = np.asarray(cents_list[a_idx], dtype=np.float32).astype('<f4', copy=False)\n",
    "\n",
    "            # Write raw bytes\n",
    "            amp_arr.tofile(f_amps)\n",
    "            cent_arr.tofile(f_cents)\n",
    "\n",
    "            a_entry = {\n",
    "                \"id\": f\"ex{ex_idx}_a{a_idx:03d}\",\n",
    "                \"onset_time\":   on_t, \n",
    "                \"sustain_time\": sus_t, \n",
    "                \"amp_off\":  int(amp_off),\n",
    "                \"amp_len\":  int(amp_arr.size),\n",
    "                \"cent_off\": int(cent_off),\n",
    "                \"cent_len\": int(cent_arr.size),\n",
    "            }\n",
    "\n",
    "            amp_off  += amp_arr.size\n",
    "            cent_off += cent_arr.size\n",
    "\n",
    "            ex_entry[\"articulations\"].append(a_entry)\n",
    "\n",
    "        manifest[\"exercises\"].append(ex_entry)\n",
    "\n",
    "    f_amps.close()\n",
    "    f_cents.close()\n",
    "\n",
    "    with open(os.path.join(out_dir, \"pack.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "def create_bin(exercises):\n",
    "    # AB project graph length in pixels (for efficient compression)\n",
    "    graph_size = 2 ** 10\n",
    "\n",
    "    # format: a list for each exercise, each element is [[onsets], [sustains], [amps], [centroids]]\n",
    "    # onsets is a list of onsets\n",
    "    # sustains is a list of sustains\n",
    "    # amps is a list of list of amplitudes corresponding to each onset/sustain\n",
    "    # centroids is a list of list of centroids corresponding to each onset/sustain\n",
    "    data = []\n",
    "    for ex in exercises:\n",
    "        exercise = ex.name\n",
    "        spec_thresh = ex.spec_thresh\n",
    "        min_time_between = ex.min_time_between * .8\n",
    "        sustain_thresh = ex.sustain_thresh_coeff * spec_thresh\n",
    "\n",
    "        ys, ts, sr = helpers.get_audio_data(f\"exercises/{exercise}\")\n",
    "        ts_fr, fr_freq_amps, freq_bins = helpers.magnitude_spectrogram(ys, ts, sr)\n",
    "        ts_fr, spec_flux = helpers.compute_spectral_flux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "        centroids = helpers.compute_spectral_centroid(time_frames=ts_fr, freq_bins=freq_bins, frame_freq_amps=fr_freq_amps)        \n",
    "        onsets, sustains = helpers.detect_onsets_and_release(spec_flux=spec_flux, times=ts_fr, sr=sr, sustain_thresh=sustain_thresh, onset_thresh=spec_thresh, min_time_between=min_time_between)\n",
    "\n",
    "        data_onsets, data_sustains, data_smoothed_amps, data_centroids = [], [], [], []\n",
    "        for i in range(min(len(onsets), len(sustains))):\n",
    "            onset_i = bisect_left(ts, onsets[i])\n",
    "            sustain_i = bisect_left(ts, sustains[i])\n",
    "            amps_window = np.linspace(onset_i, sustain_i, num=graph_size, dtype=int)\n",
    "            # smoothed amplitudes in this window\n",
    "            smooth_window_size = 15\n",
    "            smoothed_amps = np.convolve(ys[amps_window], np.ones(smooth_window_size) / smooth_window_size, mode='same')\n",
    "\n",
    "\n",
    "            onset_i_ts_fr = bisect_left(ts_fr, onsets[i])\n",
    "            sustain_i_ts_fr = bisect_left(ts_fr, sustains[i])\n",
    "            centroids_window = np.linspace(onset_i_ts_fr, sustain_i_ts_fr, num=graph_size, dtype=int)\n",
    "            centrds = centroids[centroids_window]\n",
    "\n",
    "            data_onsets.append(ts[onset_i])\n",
    "            data_sustains.append(ts[sustain_i])\n",
    "            data_smoothed_amps.append(smoothed_amps)\n",
    "            data_centroids.append(centrds)\n",
    "        data.append([data_onsets, data_sustains, data_smoothed_amps, data_centroids])\n",
    "    \n",
    "    pack_articulations(data, \"precomputed_target_bin\")\n",
    "\n",
    "create_bin(opt_exercises)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d863e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "from numpy.fft import fft, fftshift, fftfreq\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b363c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For exercise ex1WholeMod.mp4, Note count Actual:\t90, Note count Detected: \t88 | Accuracy: \t0.9777777777777777\n",
      "For ex1WholeMod.mp4, mean articulation length: 0.0329545454545455\n",
      "For exercise ex1WholeModF.mp4, Note count Actual:\t90, Note count Detected: \t90 | Accuracy: \t1.0\n",
      "For ex1WholeModF.mp4, mean articulation length: 0.032888888888888926\n",
      "For exercise ex2WholeMod.mp4, Note count Actual:\t49, Note count Detected: \t46 | Accuracy: \t0.9387755102040817\n",
      "For ex2WholeMod.mp4, mean articulation length: 0.031086956521739116\n",
      "For exercise ex3WholeMod.mp4, Note count Actual:\t145, Note count Detected: \t142 | Accuracy: \t0.9793103448275862\n",
      "For ex3WholeMod.mp4, mean articulation length: 0.0379577464788733\n",
      "For exercise ex3WholeModF.mp4, Note count Actual:\t145, Note count Detected: \t141 | Accuracy: \t0.9724137931034482\n",
      "For ex3WholeModF.mp4, mean articulation length: 0.036170212765957416\n",
      "For exercise ex4WholeMod.mp4, Note count Actual:\t102, Note count Detected: \t106 | Accuracy: \t0.9622641509433962\n",
      "For ex4WholeMod.mp4, mean articulation length: 0.035849056603773695\n",
      "For exercise ex4WholeModF.mp4, Note count Actual:\t102, Note count Detected: \t90 | Accuracy: \t0.8823529411764706\n",
      "For ex4WholeModF.mp4, mean articulation length: 0.042666666666666825\n",
      "For exercise ex5WholeMod.mp4, Note count Actual:\t133, Note count Detected: \t121 | Accuracy: \t0.9097744360902256\n",
      "For ex5WholeMod.mp4, mean articulation length: 0.028429752066115872\n",
      "For exercise ex5WholeModF.mp4, Note count Actual:\t133, Note count Detected: \t125 | Accuracy: \t0.9398496240601504\n",
      "For ex5WholeModF.mp4, mean articulation length: 0.029920000000000137\n",
      "For exercise ex6WholeMod.mp4, Note count Actual:\t118, Note count Detected: \t118 | Accuracy: \t1.0\n",
      "For ex6WholeMod.mp4, mean articulation length: 0.036694915254237445\n",
      "For exercise ex6WholeModF.mp4, Note count Actual:\t118, Note count Detected: \t118 | Accuracy: \t1.0\n",
      "For ex6WholeModF.mp4, mean articulation length: 0.0391525423728811\n",
      "For exercise ex7WholeMod.mp4, Note count Actual:\t86, Note count Detected: \t83 | Accuracy: \t0.9651162790697675\n",
      "For ex7WholeMod.mp4, mean articulation length: 0.03325301204819304\n",
      "For exercise ex8WholeMod.mp4, Note count Actual:\t112, Note count Detected: \t112 | Accuracy: \t1.0\n",
      "For ex8WholeMod.mp4, mean articulation length: 0.03625000000000016\n",
      "For exercise ex8WholeModF.mp4, Note count Actual:\t112, Note count Detected: \t112 | Accuracy: \t1.0\n",
      "For ex8WholeModF.mp4, mean articulation length: 0.037678571428571596\n",
      "For exercise ex9WholeMod.mp4, Note count Actual:\t121, Note count Detected: \t119 | Accuracy: \t0.9834710743801653\n",
      "For ex9WholeMod.mp4, mean articulation length: 0.03630252100840335\n",
      "For exercise ex10WholeMod.mp4, Note count Actual:\t102, Note count Detected: \t102 | Accuracy: \t1.0\n",
      "For ex10WholeMod.mp4, mean articulation length: 0.039803921568627505\n",
      "Average articulation window size is0.03544120682046719\n"
     ]
    }
   ],
   "source": [
    "import helpers\n",
    "importlib.reload(helpers)\n",
    "\n",
    "# Driver\n",
    "class Ex:\n",
    "    def __init__(self, name: str, num_notes: int, spec_thresh: float, bpm: int, max_num_notes_per_beat: int):\n",
    "        self.name = name\n",
    "        self.num_notes = num_notes\n",
    "        self.spec_thresh = spec_thresh\n",
    "        self.bpm = bpm\n",
    "        self.max_num_notes_per_beat = max_num_notes_per_beat\n",
    "        self.min_time_between = 60 / (self.bpm * max_num_notes_per_beat)\n",
    "\n",
    "exerciseNoteCounts = [\n",
    "    Ex(\"ex1WholeMod.mp4\", num_notes=90, spec_thresh=.15, bpm=80, max_num_notes_per_beat=2),\n",
    "    Ex(\"ex1WholeModF.mp4\", num_notes=90, spec_thresh=.15, bpm=120, max_num_notes_per_beat=2),\n",
    "    Ex(\"ex2WholeMod.mp4\", num_notes=49, spec_thresh=.15, bpm=80, max_num_notes_per_beat=1),\n",
    "    Ex(\"ex3WholeMod.mp4\", num_notes=145, spec_thresh=.15, bpm=100, max_num_notes_per_beat=2),\n",
    "    Ex(\"ex3WholeModF.mp4\", num_notes=145, spec_thresh=.15, bpm=130, max_num_notes_per_beat=2),\n",
    "    Ex(\"ex4WholeMod.mp4\", num_notes=102, spec_thresh=.15, bpm=90, max_num_notes_per_beat=4),\n",
    "    Ex(\"ex4WholeModF.mp4\", num_notes=102, spec_thresh=.05, bpm=120, max_num_notes_per_beat=2),\n",
    "    Ex(\"ex5WholeMod.mp4\", num_notes=133, spec_thresh=.15, bpm=64, max_num_notes_per_beat=2),\n",
    "    Ex(\"ex5WholeModF.mp4\", num_notes=133, spec_thresh=.15, bpm=86, max_num_notes_per_beat=2),\n",
    "    Ex(\"ex6WholeMod.mp4\", num_notes=118, spec_thresh=.15, bpm=70, max_num_notes_per_beat=4),\n",
    "    Ex(\"ex6WholeModF.mp4\", num_notes=118, spec_thresh=.15, bpm=90, max_num_notes_per_beat=4),\n",
    "    Ex(\"ex7WholeMod.mp4\", num_notes=86, spec_thresh=.15, bpm=70, max_num_notes_per_beat=2),\n",
    "    Ex(\"ex8WholeMod.mp4\", num_notes=112, spec_thresh=.15, bpm=55, max_num_notes_per_beat=4),\n",
    "    Ex(\"ex8WholeModF.mp4\", num_notes=112, spec_thresh=.15, bpm=80, max_num_notes_per_beat=4),\n",
    "    Ex(\"ex9WholeMod.mp4\", num_notes=121, spec_thresh=.15, bpm=100, max_num_notes_per_beat=4),\n",
    "    Ex(\"ex10WholeMod.mp4\", num_notes=102, spec_thresh=.15, bpm=120, max_num_notes_per_beat=3)\n",
    "]\n",
    "\n",
    "avg_artic_leng = 0\n",
    "for ex in exerciseNoteCounts:\n",
    "    exercise = ex.name\n",
    "    note_count = ex.num_notes\n",
    "    spec_thresh = ex.spec_thresh\n",
    "    min_time_between = ex.min_time_between * .8\n",
    "\n",
    "    ys, ts, sr = helpers.get_audio_data(f\"exercises/{exercise}\")\n",
    "    ts_fr, fr_freq_amps, freq_bins = helpers.magnitude_spectrogram(ys, ts, sr)\n",
    "    ts_fr, spec_flux = helpers.compute_spectral_flux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "    centroids = helpers.compute_spectral_centroid(time_frames=ts_fr, freq_bins=freq_bins, frame_freq_amps=fr_freq_amps)\n",
    "\n",
    "    \n",
    "    plt.title(exercise)\n",
    "    plt.xlabel(\"Time (s)\"); plt.ylabel(\"Spectral Flux\")\n",
    "    plt.plot(ts_fr, spec_flux, c=\"orange\")\n",
    "    img_path = Path.cwd() / \"spec_flux_graphs\" / f\"{exercise} Spectral Flux.png\"\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()\n",
    "    \n",
    "    # onsets = helpers.detect_onsets(spec_flux, ts_fr, threshold=spec_thresh, min_time_between=min_time_between)\n",
    "    onsets, sustains = helpers.detect_onsets_and_release(spec_flux=spec_flux, times=ts_fr, sr=sr, onset_thresh=spec_thresh, min_time_between=min_time_between)\n",
    "\n",
    "    print_num_onset_detection_accuracy = True\n",
    "    graph_onsets_sustains = True\n",
    "    \n",
    "    if print_num_onset_detection_accuracy:\n",
    "        print(f\"For exercise {exercise}, Note count Actual:\\t{note_count}, Note count Detected: \\t{len(onsets)} | Accuracy: \\t{min(note_count / len(onsets), len(onsets) / note_count)}\")\n",
    "\n",
    "    if graph_onsets_sustains:\n",
    "        artic_lens = [sustains[i] - onsets[i] for i in range(min(len(onsets), len(sustains)))]\n",
    "        # if np.max(artic_lens) > 1:\n",
    "        # print(f\"For {exercise}, min articulation length: {np.min(artic_lens)}\")\n",
    "        # if np.max(artic_lens) > .1:\n",
    "        #     print(\"==============================================================\")\n",
    "        #     print(f\"For {exercise}, max articulation length: {np.max(artic_lens)}\")\n",
    "        print(f\"For {exercise}, mean articulation length: {np.average(artic_lens)}\")\n",
    "        avg_artic_leng += np.average(artic_lens)\n",
    "\n",
    "        onset_indic = np.zeros_like(ts_fr)\n",
    "        sustain_indic = np.zeros_like(ts_fr)\n",
    "\n",
    "        stem_top = np.max(centroids)\n",
    "\n",
    "        # Set 1.0 at the closest time points where onsets/sustains occur\n",
    "        for onset_time in onsets:\n",
    "            idx = np.argmin(np.abs(ts_fr - onset_time))\n",
    "            onset_indic[idx] = stem_top\n",
    "            \n",
    "        for sustain_time in sustains:\n",
    "            idx = np.argmin(np.abs(ts_fr - sustain_time))\n",
    "            sustain_indic[idx] = stem_top\n",
    "\n",
    "        plt.title(f\"{exercise} Onset + Sustain + Spectral Centroid\")\n",
    "        plt.xlim(10, 14)\n",
    "        plt.xlabel(\"Times (s)\")\n",
    "        plt.stem(ts_fr, onset_indic, linefmt='--', markerfmt='pink', label='Onsets')\n",
    "        plt.stem(ts_fr, sustain_indic, linefmt='--', markerfmt='red', label='Sustains')\n",
    "        plt.legend()\n",
    "        img_path = Path.cwd() / \"centroids\" / f\"{exercise} centroid.png\"\n",
    "        plt.plot(ts_fr, centroids)\n",
    "        plt.savefig(img_path)\n",
    "        plt.close()\n",
    "\n",
    "print(f\"Average articulation window size is{avg_artic_leng / len(exerciseNoteCounts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "977a7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_spec_thresh():\n",
    "    lowest, highest, num_tests = .01, .25, 100\n",
    "    spec_thres_vals = np.linspace(lowest, highest, num_tests)\n",
    "    ret = [] # the optimized thresholds for each exercise\n",
    "\n",
    "    for ex in exerciseNoteCounts:\n",
    "        exercise = ex.name\n",
    "        note_count = ex.num_notes\n",
    "        min_time_between = ex.min_time_between * .8\n",
    "\n",
    "        num_onsets_detected = []\n",
    "        accuracies = []\n",
    "\n",
    "        ys, ts, sr = helpers.get_audio_data(f\"exercises/{exercise}\")\n",
    "        ts_fr, fr_freq_amps, _ = helpers.magnitude_spectrogram(ys, ts, sr)\n",
    "        ts_fr, spec_flux = helpers.compute_spectral_flux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "        for spec_thresh in spec_thres_vals:\n",
    "            onsets, sustains = helpers.detect_onsets_and_release(spec_flux=spec_flux, times=ts_fr, sr=sr, onset_thresh=spec_thresh, sustain_thresh=.13, min_time_between=min_time_between)\n",
    "            accuracy = min(note_count / len(onsets), len(onsets) / note_count)\n",
    "\n",
    "            num_onsets_detected.append(len(onsets))\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "        plt.title(f\"{exercise} Spectral Flux Threshold vs Number of Onsets Detected\")\n",
    "        plt.xlabel(\"Spectral Flux Threshold\"); plt.ylabel(\"Number of Onsets Detected\")\n",
    "        plt.plot(spec_thres_vals, num_onsets_detected, label=\"Threshold vs Onsets\")\n",
    "        plt.axhline(y=note_count, color='limegreen', linestyle='--', linewidth=2, label=\"Correct # Onsets\")\n",
    "        plt.legend()\n",
    "        img_path = Path.cwd() / \"spec_thresh_opt\" / \"graphs\" / f\"{exercise}.png\"\n",
    "        plt.savefig(img_path)\n",
    "        plt.close()\n",
    "\n",
    "        opt_perf = max(accuracies)\n",
    "        opt_perf_spec_thresh = [spec_thres_vals[i] for i, val in enumerate(accuracies) if val == opt_perf]\n",
    "        with open(\"spec_thresh_opt/optimized_values.txt\", \"a\") as f:\n",
    "            f.write(f\"{'=' * 30}\\n\")\n",
    "            f.write(f\"Best performing spectral thresholds at {opt_perf * 100}% accuracy for {exercise}:\\n\")\n",
    "            f.write(f\"{opt_perf_spec_thresh}\\n\\n\")\n",
    "\n",
    "        ret.append(opt_perf_spec_thresh[len(opt_perf_spec_thresh) // 2]) # choose the middle most successful output to maximize applicability\n",
    "        \n",
    "    return ret\n",
    "\n",
    "def apply_optimized_spec_thresh(opts):\n",
    "    for i, ex in enumerate(exerciseNoteCounts):\n",
    "        ex.spec_thresh = opts[i]\n",
    "\n",
    "opt_spec_thresh = optimize_spec_thresh()\n",
    "apply_optimized_spec_thresh(opt_spec_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7567b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586),\n",
       " np.float64(0.008585858585858586)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def optimize_sustain_thresh_coeff():\n",
    "    lowest, highest, num_tests = 0, .85, 100\n",
    "    sustain_thresh_coeffs = np.linspace(lowest, highest, num_tests)\n",
    "    ret = [] # the optimized coefficients for each exercise\n",
    "\n",
    "    for ex in exerciseNoteCounts:\n",
    "        exercise = ex.name\n",
    "        note_count = ex.num_notes\n",
    "        spec_thresh = ex.spec_thresh\n",
    "        min_time_between = ex.min_time_between * .8\n",
    "\n",
    "        sustain_saturations = []\n",
    "\n",
    "        ys, ts, sr = helpers.get_audio_data(f\"exercises/{exercise}\")\n",
    "        ts_fr, fr_freq_amps, _ = helpers.magnitude_spectrogram(ys, ts, sr)\n",
    "        ts_fr, spec_flux = helpers.compute_spectral_flux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "        for coeff in sustain_thresh_coeffs:\n",
    "            sustain_thresh = spec_thresh * coeff\n",
    "            onsets, sustains = helpers.detect_onsets_and_release(spec_flux=spec_flux, times=ts_fr, sr=sr, onset_thresh=spec_thresh, sustain_thresh=sustain_thresh, min_time_between=min_time_between)\n",
    "\n",
    "            sustain_saturations.append(len(sustains) / len(onsets))\n",
    "\n",
    "        plt.title(f\"{exercise} Coefficient for Sustain vs Onset Pairing Completeness\")\n",
    "        plt.xlabel(\"Coefficient for Sustain\"); plt.ylabel(\"Onset Pairing Completeness\")\n",
    "        plt.plot(sustain_thresh_coeffs, sustain_saturations)\n",
    "        img_path = Path.cwd() / \"sustain_thresh_opt\" / \"graphs\" / f\"{exercise}.png\"\n",
    "        plt.savefig(img_path)\n",
    "        plt.close()\n",
    "\n",
    "        opt_perf_sustain_coeff = [sustain_thresh_coeffs[i] for i, val in enumerate(sustain_saturations) if val == 1]\n",
    "        with open(\"sustain_thresh_opt/optimized_values.txt\", \"a\") as f:\n",
    "            f.write(f\"{'=' * 30}\\n\")\n",
    "            f.write(f\"Minimum coefficient for complete onset + sustain pairing for {exercise}:\\n\")\n",
    "            f.write(f\"{np.min(opt_perf_sustain_coeff)}\\n\\n\")\n",
    "\n",
    "        ret.append(np.min(opt_perf_sustain_coeff)) # choose the middle most successful output to maximize applicability\n",
    "        \n",
    "    return ret\n",
    "\n",
    "# for ex in exerciseNoteCounts:\n",
    "#     print(f\"for {ex.name}: {ex.spec_thresh}\")\n",
    "\n",
    "optimize_sustain_thresh_coeff() \n",
    "# Conclusion:\n",
    "# The sustain threshold can be essentially as small a nonzero value as you'd like \n",
    "# So in practice can `sustain_thresh = onset_thresh * .01` should suffice \n",
    "# This is overall great news because it indicates that using spectral flux for \n",
    "# onset detection + sustain phase detection is a great approach, because\n",
    "# there's a ton of granularity. The spectral flux goes down to essentially 0 (very accurate)\n",
    "# Of course, this is probably partially due to smoothing, but still a good result \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

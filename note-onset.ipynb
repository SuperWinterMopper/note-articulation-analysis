{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d863e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "from numpy.fft import fft, fftshift, fftfreq\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import ffmpeg, librosa\n",
    "\n",
    "def get_audio_data(mp4_file):\n",
    "    probe = ffmpeg.probe(mp4_file)\n",
    "    audio_stream = next((stream for stream in probe['streams'] \n",
    "                        if stream['codec_type'] == 'audio'), None)\n",
    "    \n",
    "    if audio_stream is None:\n",
    "        raise ValueError(f\"No audio stream found in {mp4_file}\")\n",
    "    \n",
    "    sr = int(audio_stream['sample_rate'])\n",
    "    channels = int(audio_stream['channels'])\n",
    "    \n",
    "    out, _ = (\n",
    "        ffmpeg\n",
    "        .input(mp4_file)\n",
    "        .output('-', format='s16le', acodec='pcm_s16le')\n",
    "        .run(capture_stdout=True, capture_stderr=True)\n",
    "    )\n",
    "    \n",
    "    audio_array = np.frombuffer(out, dtype=np.int16)\n",
    "    \n",
    "    if channels > 1:\n",
    "        audio_array = audio_array.reshape(-1, channels)\n",
    "        ys = audio_array.mean(axis=1)\n",
    "    else:\n",
    "        ys = audio_array\n",
    "    \n",
    "    ys = ys / np.iinfo(np.int16).max\n",
    "    \n",
    "    num_samples = len(ys)\n",
    "    ts = np.arange(num_samples) / sr\n",
    "    \n",
    "    return ys, ts, sr\n",
    "\n",
    "def plot_fft(xs, ys):\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"FFT\")\n",
    "    plt.xlim(0, 3000)\n",
    "    plt.plot(xs, np.abs(ys))\n",
    "    plt.savefig(\"FFT Plot\")\n",
    "\n",
    "def plotMagSpec(time_frames, freq_bins, frame_freq_amps):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(211)\n",
    "    plt.pcolormesh(time_frames, freq_bins, frame_freq_amps, shading='gouraud')\n",
    "    plt.title('Magnitude Spectrogram')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.colorbar()\n",
    "    plt.ylim(0, 3000)\n",
    "\n",
    "def get_fft(ys, sr):\n",
    "    n = len(ys)\n",
    "    \n",
    "    window = np.hamming(n)\n",
    "\n",
    "    ys *= window\n",
    "\n",
    "    scale = 1.0 / np.sum(window) * n\n",
    "\n",
    "    amps = fft(ys) / n\n",
    "    amps[1:n//2] *= 2\n",
    "    amps = amps[:n//2]\n",
    "    amps *= scale\n",
    "    freq_bins = fftfreq(n, d=1/sr)[:n//2]\n",
    "    return freq_bins, amps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc5f60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitudeSpectrogram(ys, ts, sr):\n",
    "    fr_len = .002 # 20ms frames\n",
    "    hop_length = int(sr * fr_len)\n",
    "    \n",
    "    frame_freq_amps = []\n",
    "    time_frames = []\n",
    "\n",
    "    # Process first frame to get dimensions\n",
    "    frame_ys = ys[0:hop_length]\n",
    "    freq_bins, _ = get_fft(frame_ys, sr)\n",
    "    \n",
    "    for i in range(0, len(ys) - hop_length + 1, hop_length // 2):\n",
    "        frame_ys = ys[i:i + hop_length].copy()\n",
    "        _, amps = get_fft(frame_ys, sr)\n",
    "        frame_freq_amps.append(np.abs(amps))\n",
    "        time_frames.append(ts[i + hop_length // 2])\n",
    "    \n",
    "    time_frames = np.array(time_frames)\n",
    "    frame_freq_amps = np.column_stack(frame_freq_amps)\n",
    "    # plotMagSpec(time_frames=time_frames, freq_bins=freq_bins, frame_freq_amps=frame_freq_amps)\n",
    "    return time_frames, frame_freq_amps\n",
    "\n",
    "def computeSpectralFlux(time_frames, frame_freq_amps, sample_rate):\n",
    "    # note that frame_freq_amps is already magnitude\n",
    "    spec_flux = np.zeros(len(time_frames) - 1)\n",
    "\n",
    "    assert(len(time_frames) == len(frame_freq_amps[0]))\n",
    "\n",
    "    for t in range(1, len(time_frames)):\n",
    "        flux = 0\n",
    "        for f in range(len(frame_freq_amps)):\n",
    "            # we half-wave rectify, so this only computes positive changes in\n",
    "            flux += frame_freq_amps[f][t] - frame_freq_amps[f][t - 1] \n",
    "        spec_flux[t - 1] = flux\n",
    "\n",
    "    spec_flux = np.array(spec_flux)\n",
    "\n",
    "    # spec_flux = gaussian_filter1d(spec_flux, sigma=1, truncate=3)\n",
    "    \n",
    "    # apply filter to smooth. use median filter with window ~40 ms.\n",
    "    frame_hz = (time_frames[1] - time_frames[0]) * sample_rate\n",
    "    k = int(round(.04 * frame_hz)) | 1\n",
    "    spec_flux = medfilt(spec_flux, kernel_size=5)\n",
    "\n",
    "    # half-wave rectify\n",
    "    spec_flux = np.maximum(spec_flux, 0)\n",
    "    \n",
    "    # Normalize spec_flux\n",
    "    spec_flux /= max(spec_flux)\n",
    "\n",
    "\n",
    "    return time_frames[1:], spec_flux # skip the first in time_frames due to difference calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831fa224",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (334453974.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 32\u001b[1;36m\u001b[0m\n\u001b[1;33m    f0_diff = voiced[f0_frame] and all([all(row) for row in diffs]) and\u001b[0m\n\u001b[1;37m                                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def differentiate(ys, xs):\n",
    "    return np.diff(ys) / np.diff(xs)\n",
    "\n",
    "# hop_len is the hop_len for librosa.pyin\n",
    "def detect_onsets(spec_flux, times, sr, f0, voiced, hop_len, threshold=0.15, min_time_between=0.15, min_hz_diff=80):\n",
    "    onsets = []\n",
    "    \n",
    "    # Find local maxima above threshold \n",
    "    for i in range(1, len(spec_flux)-1): \n",
    "        # Check timing with previous onset\n",
    "        if not onsets or (times[i] - onsets[-1]) > min_time_between:\n",
    "            # Check if it's a local peak above threshold \n",
    "            flux_peak = (spec_flux[i] > spec_flux[i-1] and spec_flux[i] >= spec_flux[i+1] and spec_flux[i] > threshold)\n",
    "\n",
    "\n",
    "            # check for note fundamental frequency change (f0)\n",
    "            f0_frame = int(i / hop_len)\n",
    "            seconds_per_f0_read = hop_len / sr\n",
    "            \n",
    "            # number of f0 reads for chunk of time over min_time_between\n",
    "            num_f0s = min_time_between / seconds_per_f0_read\n",
    "\n",
    "            window_size = max(1, int(num_f0s))\n",
    "            window_left = np.arange(max(0, f0_frame - window_size), max(0, f0_frame - 1))\n",
    "            window_right = np.arange(f0_frame, min(len(f0) - 1, f0_frame + window_size))\n",
    "\n",
    "            # check that all f0 on right and left window are sufficiently different\n",
    "            lr_diff = [[abs(f0[right] - f0[left]) >= min_hz_diff for left in window_left] for right in window_right]\n",
    "            lf_window_diff: bool = all([all(row) for row in lr_diff])\n",
    "\n",
    "            r_window_vals = [f0[i] for i in window_right]\n",
    "            all_r_same: bool = max(r_window_vals) - min(r_window_vals) < min_hz_diff\n",
    "\n",
    "            # check that all values in the right window are sufficiently similar\n",
    "            \n",
    "            f0_diff = voiced[f0_frame] and lf_window_diff and all_r_same\n",
    "\n",
    "            if flux_peak or f0_diff:\n",
    "                onsets.append(times[i])\n",
    "\n",
    "    return onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecc8820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 118 notes articulated within this\n",
    "# ys, ts, sr = get_audio_data(\"ex6WholeModF.mp4\")\n",
    "\n",
    "# ts_fr, fr_freq_amps = magnitudeSpectrogram(ys, ts, sr)\n",
    "# ts_fr, spec_flux = computeSpectralFlux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "# plt.plot(ts_fr, spec_flux)\n",
    "# plt.title(\"Spectral Flux Gaussian Smoothing\")\n",
    "# for i in range(4):\n",
    "#     plt.xlim(i * 5, (i + 1) * 5)\n",
    "#     plt.savefig(f\"Spectral Flux Gaussian Smoothing, Window {i * 5, (i + 1) * 5}.png\")\n",
    "# plt.close()\n",
    "\n",
    "\n",
    "# plt.title(\"Spectral Flux Derivative (approximate)\")\n",
    "# spec_flux_derivative = differentiate(spec_flux, ts_fr)\n",
    "\n",
    "# print(f\"before gaussian: {len(spec_flux_derivative)}\")\n",
    "# # apply Gaussian filter to smooth\n",
    "# spec_flux_derivative = gaussian_filter1d(spec_flux_derivative, sigma=1, truncate=3)\n",
    "# print(f\"after gaussian: {len(spec_flux_derivative)}\")\n",
    "\n",
    "# plt.plot(ts_fr[1:], spec_flux_derivative, c=\"orange\")\n",
    "\n",
    "# for i in range(4):\n",
    "#     plt.xlim((i * 5, (i+1) * 5))\n",
    "#     plt.savefig(f\"Spectral Flux Derivative (approximate) xlim {i * 5, (i+1) * 5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25675e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afkhe\\AppData\\Local\\Temp\\ipykernel_21376\\3797866710.py:44: UserWarning: With fmin=164.814, sr=48000 and frame_length=512, less than two periods of fmin fit into the frame, which can cause inaccurate pitch detection. Consider increasing to fmin=187.500 or frame_length=585.\n",
      "  f0, voiced, _ = librosa.pyin(y=ys, sr=sr, fmin=librosa.note_to_hz('E3'), fmax=librosa.note_to_hz('C6'), frame_length=f0_frame_size, hop_length=f0_hop_len)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m f0, voiced, _ \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mpyin(y\u001b[38;5;241m=\u001b[39mys, sr\u001b[38;5;241m=\u001b[39msr, fmin\u001b[38;5;241m=\u001b[39mlibrosa\u001b[38;5;241m.\u001b[39mnote_to_hz(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE3\u001b[39m\u001b[38;5;124m'\u001b[39m), fmax\u001b[38;5;241m=\u001b[39mlibrosa\u001b[38;5;241m.\u001b[39mnote_to_hz(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC6\u001b[39m\u001b[38;5;124m'\u001b[39m), frame_length\u001b[38;5;241m=\u001b[39mf0_frame_size, hop_length\u001b[38;5;241m=\u001b[39mf0_hop_len)\n\u001b[0;32m     45\u001b[0m f0 \u001b[38;5;241m=\u001b[39m gaussian_filter1d(f0, sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m onsets \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_onsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec_flux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_fr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspec_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_time_between\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_time_between\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoiced\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvoiced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf0_hop_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor exercise \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexercise\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnote_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(onsets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m, in \u001b[0;36mdetect_onsets\u001b[1;34m(spec_flux, times, sr, f0, voiced, hop_len, threshold, min_time_between, min_hz_diff, required_hz_diff_sustain)\u001b[0m\n\u001b[0;32m     24\u001b[0m window_right \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(f0_frame, f0_frame \u001b[38;5;241m+\u001b[39m half_window_size, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     26\u001b[0m diffs \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mabs\u001b[39m(f0[right] \u001b[38;5;241m-\u001b[39m f0[left]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_hz_diff \u001b[38;5;28;01mfor\u001b[39;00m left \u001b[38;5;129;01min\u001b[39;00m window_left] \u001b[38;5;28;01mfor\u001b[39;00m right \u001b[38;5;129;01min\u001b[39;00m window_right]\n\u001b[1;32m---> 28\u001b[0m f0_diff \u001b[38;5;241m=\u001b[39m \u001b[43mvoiced\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf0_frame\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[38;5;28mall\u001b[39m(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m diffs])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flux_peak \u001b[38;5;129;01mor\u001b[39;00m f0_diff:\n\u001b[0;32m     31\u001b[0m     onsets\u001b[38;5;241m.\u001b[39mappend(times[i])\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# ys, ts, sr = get_audio_data(\"ex6_90bpm_1.wav\")\n",
    "# there are 118 notes articulated within this\n",
    "\n",
    "# onsets = detect_onsets(spec_flux, ts_fr)\n",
    "# print(onsets)\n",
    "# print(len(onsets))\n",
    "\n",
    "\n",
    "exerciseNoteCounts = [\n",
    "    [\"ex1WholeMod.mp4\", 90, .3,  .4],\n",
    "    [\"ex1WholeModF.mp4\", 90, .3, .4],\n",
    "    [\"ex2WholeMod.mp4\", 49, .15, 0.15],\n",
    "    [\"ex3WholeMod.mp4\", 145, .15, 0.15],\n",
    "    [\"ex3WholeModF.mp4\", 145, .15, 0.15],\n",
    "    [\"ex4WholeMod.mp4\", 102, .15, 0.15],\n",
    "    [\"ex4WholeModF.mp4\", 102, .15, 0.15],\n",
    "    [\"ex5WholeMod.mp4\", 133, .15, 0.15],\n",
    "    [\"ex5WholeModF.mp4\", 133, .15, 0.15],\n",
    "    [\"ex6WholeMod.mp4\", 118, .15, 0.15],\n",
    "    [\"ex6WholeModF.mp4\", 118, .15, 0.15],\n",
    "    [\"ex7WholeMod.mp4\", 86, .15, 0.15],\n",
    "    [\"ex8WholeMod.mp4\", 112, .15, 0.15],\n",
    "    [\"ex8WholeModF.mp4\", 112, .15, 0.15],\n",
    "    [\"ex9WholeMod.mp4\", 121, .15, 0.15],\n",
    "    [\"ex10WholeMod.mp4\", 102, .15, 0.15]\n",
    "]\n",
    "\n",
    "for exercise, note_count, spec_thresh, min_time_between in exerciseNoteCounts:\n",
    "    ys, ts, sr = get_audio_data(exercise)\n",
    "\n",
    "    ts_fr, fr_freq_amps = magnitudeSpectrogram(ys, ts, sr)\n",
    "    ts_fr, spec_flux = computeSpectralFlux(ts_fr, fr_freq_amps, sr)\n",
    "\n",
    "\n",
    "\n",
    "    plt.title(exercise)\n",
    "    plt.xlabel(\"Time (s)\"); plt.ylabel(\"Spectral Flux\")\n",
    "    plt.plot(ts_fr, spec_flux, c=\"orange\")\n",
    "    img_path = Path.cwd() / \"spec_flux_graphs\" / f\"{exercise} Spectral Flux.png\"\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()\n",
    "    \n",
    "    f0_frame_size, f0_hop_len = 512, 2048 \n",
    "    f0, voiced, _ = librosa.pyin(y=ys, sr=sr, fmin=librosa.note_to_hz('E3'), fmax=librosa.note_to_hz('C6'), frame_length=f0_frame_size, hop_length=f0_hop_len)\n",
    "    f0 = gaussian_filter1d(f0, sigma=1)\n",
    "\n",
    "    onsets = detect_onsets(spec_flux, ts_fr, threshold=spec_thresh, min_time_between=min_time_between, f0=f0, sr=sr, voiced=voiced, hop_len=f0_hop_len)\n",
    "    print(f\"For exercise {exercise}, Actual: {note_count}, Detected: {len(onsets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812d8167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 92 notes in ex1WholeModF.mp4:\n",
      "1. A♯4 (0.03s - 0.23s, 0.20s)\n",
      "2. A4 (0.25s - 0.50s, 0.26s)\n",
      "3. A♯4 (0.51s - 0.73s, 0.21s)\n",
      "4. F4 (0.78s - 0.93s, 0.15s)\n",
      "5. C5 (0.97s - 1.19s, 0.22s)\n",
      "6. B4 (1.21s - 1.45s, 0.25s)\n",
      "7. C5 (1.46s - 1.67s, 0.21s)\n",
      "8. F4 (1.72s - 1.93s, 0.21s)\n",
      "9. D5 (1.94s - 2.15s, 0.21s)\n",
      "10. C♯5 (2.17s - 2.41s, 0.25s)\n",
      "11. D5 (2.42s - 2.62s, 0.20s)\n",
      "12. G4 (2.63s - 2.85s, 0.21s)\n",
      "13. C5 (2.91s - 3.19s, 0.28s)\n",
      "14. B4 (3.20s - 3.43s, 0.23s)\n",
      "15. C5 (3.45s - 3.68s, 0.23s)\n",
      "16. D♯4 (3.69s - 3.89s, 0.20s)\n",
      "17. G4 (3.93s - 4.16s, 0.23s)\n",
      "18. F♯4 (4.17s - 4.42s, 0.25s)\n",
      "19. G4 (4.43s - 4.67s, 0.25s)\n",
      "20. C4 (4.70s - 4.85s, 0.15s)\n",
      "21. G4 (4.98s - 5.19s, 0.21s)\n",
      "22. F♯4 (5.21s - 5.44s, 0.23s)\n",
      "23. G4 (5.45s - 5.64s, 0.19s)\n",
      "24. A♯4 (5.69s - 5.90s, 0.21s)\n",
      "25. A4 (5.95s - 6.18s, 0.22s)\n",
      "26. G♯4 (6.19s - 6.43s, 0.25s)\n",
      "27. A4 (6.44s - 6.62s, 0.18s)\n",
      "28. C5 (6.68s - 6.86s, 0.18s)\n",
      "29. A♯4 (6.93s - 8.06s, 1.13s)\n",
      "30. D♯5 (8.09s - 8.33s, 0.25s)\n",
      "31. D5 (8.34s - 8.54s, 0.20s)\n",
      "32. D♯5 (8.55s - 8.77s, 0.21s)\n",
      "33. F5 (8.79s - 9.03s, 0.25s)\n",
      "34. D5 (9.05s - 9.27s, 0.22s)\n",
      "35. C♯5 (9.28s - 9.50s, 0.22s)\n",
      "36. D5 (9.51s - 9.75s, 0.23s)\n",
      "37. A♯4 (9.80s - 10.02s, 0.21s)\n",
      "38. G4 (10.05s - 10.28s, 0.23s)\n",
      "39. F♯4 (10.29s - 10.54s, 0.25s)\n",
      "40. G4 (10.55s - 10.75s, 0.20s)\n",
      "41. A♯4 (10.79s - 11.05s, 0.26s)\n",
      "42. D5 (11.06s - 11.33s, 0.27s)\n",
      "43. C♯5 (11.34s - 11.55s, 0.21s)\n",
      "44. D5 (11.56s - 11.79s, 0.22s)\n",
      "45. B4 (11.80s - 12.05s, 0.26s)\n",
      "46. C5 (12.06s - 12.33s, 0.27s)\n",
      "47. B4 (12.34s - 12.59s, 0.25s)\n",
      "48. C5 (12.60s - 12.78s, 0.18s)\n",
      "49. F5 (12.83s - 13.05s, 0.21s)\n",
      "50. E5 (13.07s - 13.32s, 0.26s)\n",
      "51. D♯5 (13.33s - 13.57s, 0.23s)\n",
      "52. E5 (13.58s - 13.80s, 0.22s)\n",
      "53. G4 (13.81s - 14.01s, 0.19s)\n",
      "54. C5 (14.05s - 14.29s, 0.25s)\n",
      "55. B4 (14.30s - 14.55s, 0.25s)\n",
      "56. C5 (14.56s - 14.77s, 0.21s)\n",
      "57. E4 (14.78s - 15.00s, 0.21s)\n",
      "58. F4 (15.02s - 15.29s, 0.27s)\n",
      "59. F♯4 (15.30s - 15.49s, 0.19s)\n",
      "60. G4 (15.53s - 15.75s, 0.22s)\n",
      "61. A4 (15.79s - 15.95s, 0.16s)\n",
      "62. B4 (15.96s - 16.13s, 0.17s)\n",
      "63. A♯4 (16.14s - 16.36s, 0.22s)\n",
      "64. A4 (16.37s - 16.59s, 0.21s)\n",
      "65. A♯4 (16.60s - 16.82s, 0.22s)\n",
      "66. F4 (16.85s - 17.01s, 0.16s)\n",
      "67. C5 (17.10s - 17.34s, 0.25s)\n",
      "68. B4 (17.35s - 17.59s, 0.23s)\n",
      "69. C5 (17.60s - 17.83s, 0.23s)\n",
      "70. F4 (17.86s - 18.09s, 0.23s)\n",
      "71. D5 (18.10s - 18.33s, 0.22s)\n",
      "72. C♯5 (18.34s - 18.56s, 0.22s)\n",
      "73. D5 (18.57s - 18.79s, 0.22s)\n",
      "74. B4 (18.87s - 19.07s, 0.20s)\n",
      "75. C5 (19.08s - 19.34s, 0.26s)\n",
      "76. B4 (19.35s - 19.58s, 0.23s)\n",
      "77. C5 (19.59s - 19.81s, 0.21s)\n",
      "78. D♯4 (19.82s - 20.04s, 0.22s)\n",
      "79. G4 (20.05s - 20.29s, 0.23s)\n",
      "80. F♯4 (20.30s - 20.52s, 0.22s)\n",
      "81. G4 (20.53s - 20.78s, 0.25s)\n",
      "82. C4 (20.80s - 20.96s, 0.16s)\n",
      "83. G4 (21.09s - 21.26s, 0.17s)\n",
      "84. F4 (21.27s - 21.45s, 0.18s)\n",
      "85. E4 (21.46s - 21.64s, 0.18s)\n",
      "86. F4 (21.65s - 21.88s, 0.22s)\n",
      "87. A♯4 (21.89s - 22.06s, 0.17s)\n",
      "88. A4 (22.11s - 22.31s, 0.20s)\n",
      "89. G♯4 (22.33s - 22.58s, 0.26s)\n",
      "90. A4 (22.59s - 22.79s, 0.20s)\n",
      "91. C5 (22.82s - 23.03s, 0.21s)\n",
      "92. A♯4 (23.08s - 24.21s, 1.13s)\n"
     ]
    }
   ],
   "source": [
    "def extract_notes_from_recording(mp4_file, min_note_duration=0.1):\n",
    "    \"\"\"\n",
    "    Extract note names in order using librosa's pYIN algorithm from an mp4 recording.\n",
    "    \n",
    "    Args:\n",
    "        mp4_file: Path to the mp4 file containing audio\n",
    "        min_note_duration: Minimum duration (in seconds) for a note to be considered\n",
    "        \n",
    "    Returns:\n",
    "        List of detected notes with their timing information\n",
    "    \"\"\"\n",
    "    # Get the audio data\n",
    "    ys, ts, sr = get_audio_data(mp4_file)\n",
    "    \n",
    "    # Run the pYIN algorithm\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "        y=ys, \n",
    "        sr=sr, \n",
    "        fmin=librosa.note_to_hz('E3'), \n",
    "        fmax=librosa.note_to_hz('C6'),\n",
    "        frame_length=2048,\n",
    "        hop_length=512\n",
    "    )\n",
    "    \n",
    "    # Get time stamps for each frame\n",
    "    frame_times = librosa.frames_to_time(np.arange(len(f0)), sr=sr, hop_length=512)\n",
    "    \n",
    "    # Group continuous segments into notes\n",
    "    notes = []\n",
    "    current_note = None\n",
    "    min_frames = int(min_note_duration / (512/sr))  # Convert duration to frames\n",
    "    \n",
    "    for i in range(len(f0)):\n",
    "        if voiced_flag[i] and not np.isnan(f0[i]):\n",
    "            # Get the note name for this frequency\n",
    "            note_name = librosa.hz_to_note(f0[i], octave=True)\n",
    "            \n",
    "            # Start a new note or continue the current one\n",
    "            if current_note is None or current_note[\"name\"] != note_name:\n",
    "                # If we have a previous note, add it to our list if it's long enough\n",
    "                if current_note and (i - current_note[\"start_frame\"]) >= min_frames:\n",
    "                    notes.append({\n",
    "                        \"name\": current_note[\"name\"],\n",
    "                        \"start_time\": frame_times[current_note[\"start_frame\"]],\n",
    "                        \"end_time\": frame_times[i-1],\n",
    "                        \"duration\": frame_times[i-1] - frame_times[current_note[\"start_frame\"]],\n",
    "                        \"avg_freq\": np.mean(current_note[\"freqs\"])\n",
    "                    })\n",
    "                \n",
    "                # Start a new note\n",
    "                current_note = {\n",
    "                    \"name\": note_name,\n",
    "                    \"start_frame\": i,\n",
    "                    \"freqs\": [f0[i]]\n",
    "                }\n",
    "            else:\n",
    "                # Continue the current note\n",
    "                current_note[\"freqs\"].append(f0[i])\n",
    "    \n",
    "    # Add the final note if it exists\n",
    "    if current_note and (len(f0) - current_note[\"start_frame\"]) >= min_frames:\n",
    "        notes.append({\n",
    "            \"name\": current_note[\"name\"],\n",
    "            \"start_time\": frame_times[current_note[\"start_frame\"]],\n",
    "            \"end_time\": frame_times[-1],\n",
    "            \"duration\": frame_times[-1] - frame_times[current_note[\"start_frame\"]],\n",
    "            \"avg_freq\": np.mean(current_note[\"freqs\"])\n",
    "        })\n",
    "    \n",
    "    print(f\"Detected {len(notes)} notes in {mp4_file}:\")\n",
    "    for i, note in enumerate(notes):\n",
    "        print(f\"{i+1}. {note['name']} ({note['start_time']:.2f}s - {note['end_time']:.2f}s, {note['duration']:.2f}s)\")\n",
    "    \n",
    "    return notes\n",
    "\n",
    "# notes = extract_notes_from_recording(\"ex1WholeModF.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a56994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ts): 1162240\n",
      "len(f0): 2271\n",
      "1162752\n"
     ]
    }
   ],
   "source": [
    "ys, ts, sr = get_audio_data(\"ex1WholeModF.mp4\")\n",
    "f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "    y=ys, \n",
    "    sr=sr, \n",
    "    fmin=librosa.note_to_hz('E3'), \n",
    "    fmax=librosa.note_to_hz('C6'),\n",
    "    frame_length=2048,\n",
    "    hop_length=512\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
